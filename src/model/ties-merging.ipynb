{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import os\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_alpaca = \"DTang161/ModelMergingCode\"\n",
    "llama2 = \"meta-llama/Llama-2-13b-hf\"\n",
    "wizardlm = \"DTang161/ModelMergingLM\"\n",
    "wizardmath = \"DTang161/ModelMergingMath\"\n",
    "\n",
    "models = [code_alpaca, llama2, wizardlm, wizardmath]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d447b65f4158441685715670c28df7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/playpen-storage/vishravi/miniconda/envs/ties-merging/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/playpen-storage/vishravi/miniconda/envs/ties-merging/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/playpen-storage/vishravi/miniconda/envs/ties-merging/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/playpen-storage/vishravi/miniconda/envs/ties-merging/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7fa41117f7d467c93473c5f6fbc2642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0538bf1704844be9e8bf6808d3ae5cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "code_alpaca_model = AutoModelForCausalLM.from_pretrained(code_alpaca).state_dict()\n",
    "llama2_model = AutoModelForCausalLM.from_pretrained(llama2).state_dict()\n",
    "wizardlm = AutoModelForCausalLM.from_pretrained(wizardlm).state_dict()\n",
    "wizardmath = AutoModelForCausalLM.from_pretrained(wizardmath).state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_dict_check(state_dicts):\n",
    "    param_names = [set(model.keys()) for model in state_dicts]\n",
    "    for params in param_names[1:]:\n",
    "        if params != param_names[0]:\n",
    "            raise ValueError(\"Params do not match\")\n",
    "    return True\n",
    "\n",
    "def print_param_summary(state_dict):\n",
    "    total_params = 0\n",
    "    for name, tensor in state_dict.items():\n",
    "        print(f\"{name}: {tensor.size()}\")\n",
    "        total_params += tensor.numel()\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    \n",
    "def state_dict_to_vector(remove_keys, state_dict):\n",
    "    \"\"\"\n",
    "    Removes keys from state_dict\n",
    "    Returns vectorized state_dict which is flattened (after sorting)\n",
    "    \"\"\"\n",
    "    state_dict_copy = copy.deepcopy(state_dict)\n",
    "    print(\"copied\")\n",
    "    for key in remove_keys:\n",
    "        if key in state_dict_copy:\n",
    "            del state_dict_copy[key]\n",
    "    sorted_dict = OrderedDict(sorted(state_dict_copy.items()))\n",
    "    print(\"sorted\")\n",
    "    return torch.nn.utils.parameters_to_vector([\n",
    "        value.reshape(-1) for key, value in sorted_dict.items()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptm = llama2_model\n",
    "ftms = [code_alpaca_model, wizardlm]\n",
    "\n",
    "state_dict_check([ptm] + ftms)\n",
    "\n",
    "# print_param_summary(wizardmath)\n",
    "# print_param_summary(llama2_model)\n",
    "#state_dict_check([llama2_model] + ftms)\n",
    "\n",
    "#these vary across models so must be removed\n",
    "remove_keys = [\n",
    "    \"lm_head.weight\",\n",
    "    \"model.embed_tokens.weight\",\n",
    "]\n",
    "\n",
    "vectorized_ptm = state_dict_to_vector(remove_keys, ptm)\n",
    "# print(vectorized_ptm.shape)\n",
    "vectorized_ftms = torch.vstack([state_dict_to_vector(remove_keys, ftm) for ftm in ftms])\n",
    "# print(vectorized_ftms[0].shape)\n",
    "task_vectors = vectorized_ftms - vectorized_ptm\n",
    "\n",
    "torch.save(task_vectors, \"./tensors/task_vectors.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_top_k_mask(M, K = 20): #trim from raw vectorized models\n",
    "    d = list(M.shape)[0] # n = number of models\n",
    "    nonzero_count = int(d * (100-K)/100)\n",
    "    kth_value, _ = M.abs().kthvalue(nonzero_count, dim=0)\n",
    "    mask = M.abs() >= kth_value\n",
    "    return mask\n",
    "\n",
    "def determine_signs(trimmed_M): #elect signs from trimmed models\n",
    "    return torch.sign(trimmed_M.sum(dim=0)) #sum by parameter across models: 1 x d\n",
    "\n",
    "def create_rows_to_keep(trimmed_M, signs):\n",
    "    return torch.where(signs == 0, 0, bool((torch.sign(signs) == torch.sign(trimmed_M))))\n",
    "\n",
    "def create_selected_entries(trimmed_M, rows_to_keep):\n",
    "    return trimmed_M * rows_to_keep\n",
    "\n",
    "def create_non_zero_entries(rows_to_keep):\n",
    "    return (rows_to_keep != 0).sum(dim=0).float()\n",
    "\n",
    "def disjoint_merge(selected_entries, non_zero_count): #mean of fine-tuned task vectors matching signs\n",
    "    disjoint_mean = selected_entries.sum(dim=0) / torch.clamp(non_zero_count, min=1)\n",
    "    return disjoint_mean\n",
    "\n",
    "#masks = torch.vstack([create_top_k_mask(task_vector) for task_vector in task_vectors]) #for memory purposes split up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_mask = create_top_k_mask(task_vectors)\n",
    "trimmed_M = task_vectors * top_k_mask\n",
    "M_signs = determine_signs(trimmed_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_keep = torch.vstack([create_rows_to_keep(trimmed_M[i], M_signs[i]) for i in range(trimmed_M.shape[0])])\n",
    "torch.save(rows_to_keep, \"./tensors/rows_to_keep.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trimmed_M = torch.load(\"./tensors/trimmed_M.pt\")\n",
    "# rows_to_keep = torch.load(\"./tensors/rows_to_keep.pt\")\n",
    "\n",
    "selected_entries = trimmed_M * rows_to_keep\n",
    "torch.save(selected_entries, \"./tensors/selected_entries.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_count = create_non_zero_entries(rows_to_keep)\n",
    "torch.save(non_zero_count, \"./tensors/non_zero_count.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_entries = torch.load(\"./tensors/selected_entries.pt\")\n",
    "non_zero_count = torch.load(\"./tensors/non_zero_count.pt\")\n",
    "print(\"done loading!\")\n",
    "merged_tasks = disjoint_merge(selected_entries, non_zero_count)\n",
    "torch.save(merged_tasks, \"./tensors/merged_tasks.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tasks = torch.load(\"./tensors/merged_tasks.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add merge tensors to ptm\n",
    "\n",
    "LAMBDA = 1 #ignored for sake of computation\n",
    "\n",
    "remove_keys = [\n",
    "    \"lm_head.weight\",\n",
    "    \"model.embed_tokens.weight\",\n",
    "]\n",
    "\n",
    "\n",
    "merged_tasks = torch.load(\"./tensors/merged_tasks.pt\")\n",
    "ptm = AutoModelForCausalLM.from_pretrained(llama2).state_dict()\n",
    "print(\"loaded llama2 state dict\")\n",
    "vectorized_ptm = state_dict_to_vector(remove_keys, ptm)\n",
    "print(\"vectorized llama2)\")\n",
    "vectorized_merged = vectorized_ptm + merged_tasks #technically vectorized_ptm + LAMBDA * merged_tasks\n",
    "print(\"finished creating weights\")\n",
    "torch.save(vectorized_merged, \"./tensors/merged_model_vector.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_state_dict(vectorized_model, state_dict, remove_keys):\n",
    "    reference_dict = copy.deepcopy(state_dict)\n",
    "    removed_weights = {}\n",
    "    \n",
    "    for key in remove_keys:\n",
    "        if key in state_dict:\n",
    "            print(key)\n",
    "            removed_weights[key] = copy.deepcopy(state_dict[key])\n",
    "            del reference_dict[key]\n",
    "    sorted_reference_dict = OrderedDict(sorted(reference_dict.items()))\n",
    "    \n",
    "    torch.nn.utils.vector_to_parameters(vectorized_model, sorted_reference_dict.values())\n",
    "    \n",
    "    for key in remove_keys:\n",
    "        if key in state_dict:\n",
    "            sorted_reference_dict[key] = copy.deepcopy(removed_weights[key])\n",
    "            print(sorted_reference_dict[key])\n",
    "    \n",
    "    return sorted_reference_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ptm = AutoModelForCausalLM.from_pretrained(llama2).state_dict()\n",
    "remove_keys = [\n",
    "    \"lm_head.weight\",\n",
    "    \"model.embed_tokens.weight\",\n",
    "]\n",
    "vectorized_merged = torch.load(\"./tensors/merged_model_vector.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_summary(state_dict):\n",
    "    total_params = 0\n",
    "    for name, tensor in state_dict.items():\n",
    "        print(f\"{name}: {tensor.size()}\")\n",
    "        total_params += tensor.numel()\n",
    "    print(f\"Total Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_head.weight\n",
      "model.embed_tokens.weight\n",
      "tensor([[ 6.2561e-03, -4.3945e-03,  1.3885e-03,  ..., -1.8433e-02,\n",
      "         -1.2878e-02, -4.8523e-03],\n",
      "        [-7.3853e-03, -1.0559e-02, -1.9150e-03,  ..., -8.4686e-04,\n",
      "         -5.1498e-05, -1.4954e-02],\n",
      "        [ 1.8677e-02, -3.8300e-03,  1.6357e-02,  ..., -1.2207e-02,\n",
      "          1.9775e-02,  7.8125e-03],\n",
      "        ...,\n",
      "        [-1.8066e-02,  1.2360e-03, -5.3711e-03,  ..., -3.4912e-02,\n",
      "          2.5146e-02, -1.9043e-02],\n",
      "        [ 1.7700e-02, -1.2268e-02, -2.5635e-02,  ..., -7.7820e-03,\n",
      "          2.4170e-02,  9.0332e-03],\n",
      "        [-1.5198e-02, -1.4709e-02,  5.7068e-03,  ..., -3.5400e-02,\n",
      "         -2.0599e-03, -2.5513e-02]])\n",
      "tensor([[-4.8876e-06,  6.4969e-06,  9.5367e-07,  ...,  5.9605e-08,\n",
      "          2.8014e-06, -2.6822e-06],\n",
      "        [ 3.6316e-03,  4.2114e-03, -1.0300e-03,  ...,  3.9368e-03,\n",
      "          8.2397e-03, -5.1117e-04],\n",
      "        [-3.7575e-04, -3.1090e-04,  1.5869e-03,  ...,  2.8419e-04,\n",
      "          1.7738e-04, -7.0572e-05],\n",
      "        ...,\n",
      "        [ 1.5625e-02,  1.6357e-02,  1.8188e-02,  ..., -1.7578e-02,\n",
      "          2.5391e-02, -8.1177e-03],\n",
      "        [-1.1169e-02, -6.7139e-03, -1.8978e-04,  ...,  1.3367e-02,\n",
      "         -2.8564e-02,  3.5889e-02],\n",
      "        [ 2.4536e-02, -1.7700e-02,  7.4768e-03,  ..., -2.1484e-02,\n",
      "         -4.6692e-03,  5.9509e-03]])\n"
     ]
    }
   ],
   "source": [
    "merged_model_state_dict = vector_to_state_dict(vectorized_merged, ptm, remove_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(merged_model_state_dict, \"./merged_model_state_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634e957b4e514e98816ab4f334f2321b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model = AutoModelForCausalLM.from_pretrained(llama2)\n",
    "merged_model_state_dict = torch.load(\"./merged_model_state_dict.pth\")\n",
    "merged_model.load_state_dict(merged_model_state_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ties-merging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
